{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdsza/CUDAProg/blob/LinAlg/Cuda_Cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeQYl5po5NcS",
        "outputId": "90618c48-a997-4a47-d3c2-3b919d787ba5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting simple\n",
        "\n",
        "We'll start with a simple C++ program that adds the elements of two arrays with a million elements each"
      ],
      "metadata": {
        "id": "h0ItRSVb5NIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIqo0MmT10ia",
        "outputId": "449774ca-c492-4d62-9c9a-fc95ebd4a241"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "g++ add.cpp -o add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l-yiDm013y2",
        "outputId": "ee9e258e-7bee-40a2-dbd8-5db2c3e31237"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmJ1c17d5GJ4",
        "outputId": "3d947384-c0e1-460b-8af7-ca768c38fb20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, it prints that there was no error in the summation and then exits. Now"
      ],
      "metadata": {
        "id": "hFW1iVnz5lji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```cpp\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "102tPwVi6PZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These `__global__` functions are known as *kernels*, and code tat runs on the GPU is often called *device code*, while the code that runs on the CPU is *host code*."
      ],
      "metadata": {
        "id": "wQKLvPVa9HxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Allocation in CUDA\n",
        "\n",
        "To compute on the GPU, i need to allocate memory accessible by the GPU. [Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) in CUDA makes this easy by providing a single memory space accessible by all GPUs and CPUs in your system. To allocate data in unified memory, call `cudaMallocManaged()`, which returns a pointer that you can access from the host (CPU) code or device (GPU) code. To free the data, just pass the pointer to `cudaFree()`.\n",
        "\n",
        "I just need to replace the calls to `new` in the code above with calls to `cudaMallocManaged()`, and replace calls to `delete []` with calls to `cudaFree`."
      ],
      "metadata": {
        "id": "_ecWb9Ak78RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```cpp\n",
        "// Allocate Unified Memory -- accessible from CPU or GPU\n",
        "float *x, *y;\n",
        "cudaMallocManaged(&x, N*sizeof(float));\n",
        "cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "...\n",
        "\n",
        "//Free memory\n",
        "cudaFree(x);\n",
        "cudaFree(y);\n",
        "```"
      ],
      "metadata": {
        "id": "G1dRhbv88tZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I need to *launch* the`add()` kernel, which invokes it on the GPU. CUDA kernel launches are specific using the triple angle bracket syntax `<<< >>>`. I just have to add it to the call to `add` before the parameter list."
      ],
      "metadata": {
        "id": "wD9ZzEx69bxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```cpp\n",
        "add<<<1, 1>>>(N, x, y);\n",
        "```"
      ],
      "metadata": {
        "id": "JWvzx9Rp9vlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Easy! I'll get int the details of what goes inside the angle brackets soon; for now all you need to know is that this line launches one GPU thread to run `add()`.\n",
        "Just one more thing: I need the CPU to wait until the kernel is done before it accesses the results (because CUDA kernel launches don't block the calling CPU thread). To do this I just call `cudaDeviceSynchronize()` before doing the final error checking on the CPU.\n",
        "Here's the complete code:"
      ],
      "metadata": {
        "id": "ykKbrK-X93aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20\n",
        " ;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory â€“ accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whPAuOla5LEK",
        "outputId": "c1681c8e-44ab-4073-bed2-5669eeb5c149"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add.cu -o add_cuda\n",
        "./add_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I5XiWISC8em",
        "outputId": "2f9a3617-0bca-415b-a077-e99f591a898a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is only a first step, because as written, this kernel is only correct for a single thread, since every thread that runs it will perform the add on the whole array. Moreover, there is a race condition since multiple parallel threads would both read and write the same locations."
      ],
      "metadata": {
        "id": "N3PQ3X_sDnSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profile it!\n",
        "\n",
        "I think the simplest way to find out how long the kernels takes to run is to run it with `nvprof`, the command line GPU profiler that comes with the CUDA Toolkit. Just type `nvprof ./add_cuda` on the command line:"
      ],
      "metadata": {
        "id": "w7SEaaWKD7I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvprof ./add_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcnurvF2D503",
        "outputId": "a4ae22ae-f50f-4218-aa36-f9e9a1c774eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1778== NVPROF is profiling process 1778, command: ./add_cuda\n",
            "Max error: 0\n",
            "==1778== Profiling application: ./add_cuda\n",
            "==1778== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  110.48ms         1  110.48ms  110.48ms  110.48ms  add(int, float*, float*)\n",
            "      API calls:   64.41%  201.75ms         2  100.87ms  41.776us  201.71ms  cudaMallocManaged\n",
            "                   35.28%  110.50ms         1  110.50ms  110.50ms  110.50ms  cudaDeviceSynchronize\n",
            "                    0.17%  524.49us         2  262.24us  256.42us  268.06us  cudaFree\n",
            "                    0.09%  267.94us         1  267.94us  267.94us  267.94us  cudaLaunchKernel\n",
            "                    0.05%  165.42us       114  1.4510us     135ns  60.072us  cuDeviceGetAttribute\n",
            "                    0.00%  11.239us         1  11.239us  11.239us  11.239us  cuDeviceGetName\n",
            "                    0.00%  5.0040us         1  5.0040us  5.0040us  5.0040us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.5480us         1  4.5480us  4.5480us  4.5480us  cuDeviceTotalMem\n",
            "                    0.00%  1.6870us         3     562ns     185ns  1.2120us  cuDeviceGetCount\n",
            "                    0.00%     906ns         2     453ns     169ns     737ns  cuDeviceGet\n",
            "                    0.00%     482ns         1     482ns     482ns     482ns  cuModuleGetLoadingMode\n",
            "                    0.00%     262ns         1     262ns     262ns     262ns  cuDeviceGetUuid\n",
            "\n",
            "==1778== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  812.9480us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.1800us  Device To Host\n",
            "      12         -         -         -           -  4.218234ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above will show the single call to `add`. Your timing may vary depending on the GPU allocated to you by Colab. To see the current GPU allocated to you run the following cell and look in the `Name` column where you might see, for example `Tesla T4`:"
      ],
      "metadata": {
        "id": "c2FsvXV2Et7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIWTZ2HELN7",
        "outputId": "c9868ad2-057a-48d9-d720-94efe44d4d66"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 15 13:46:14 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make it faster with parallelism."
      ],
      "metadata": {
        "id": "2dXngjIUL4nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Picking up the Threads\n",
        "\n",
        "Now that you've run a kernel with one thread that does some computation, how do you make it parallel? The key is in CUDA's `<<<1, 1>>>` syntax. This is called the execution configuration, and it tells the CUDA runtime how many parallel threads to use for the launch on the GPU. There are two parameters here, but let's start by changing the second one: the number of threads in a thread block. CUDA GPUs run kernels using blocks of threads that are a multiple of 32 in size, so 256 threads is reasonable size to choose."
      ],
      "metadata": {
        "id": "BbUwhvstL7Or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```cpp\n",
        "add<<<1, 256>>>(N, x, y);\n",
        "```"
      ],
      "metadata": {
        "id": "xAnqk-vcMfGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If i run the code with only this change, it will do the computation once per thread, rather than the spreading the computation across the parallel threads. To do it properly, i need to modify the kernel. CUDA C++ provides keywords that let kernels get the indices of the running threads. Specifically, `threadIdx.x` contains the index of the current thread within its block, and `blockDim.x` contains the number of threads in the block. I'll just modify the loop to stride thro"
      ],
      "metadata": {
        "id": "fDpzgp2aMlrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MatrixMul.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define TILE_WIDTH 16  // Define the tile width for tiling\n",
        "\n",
        "// Kernel function for tiled matrix multiplication\n",
        "__global__ void matrixMultiplyTiled(float *d_M, float *d_N, float *d_P, int M_rows, int M_cols, int N_cols) {\n",
        "    // Shared memory arrays for tiles\n",
        "    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    // Calculate row and column indices of the element in P\n",
        "    int Row = blockIdx.y * TILE_WIDTH + threadIdx.y;\n",
        "    int Col = blockIdx.x * TILE_WIDTH + threadIdx.x;\n",
        "\n",
        "    float Pvalue = 0.0;\n",
        "\n",
        "    // Loop over all tiles\n",
        "    for (int ph = 0; ph < (M_cols + TILE_WIDTH - 1) / TILE_WIDTH; ++ph) {\n",
        "        // Load M and N tiles into shared memory\n",
        "        if (Row < M_rows && ph * TILE_WIDTH + threadIdx.x < M_cols) {\n",
        "            Mds[threadIdx.y][threadIdx.x] = d_M[Row * M_cols + ph * TILE_WIDTH + threadIdx.x];\n",
        "        } else {\n",
        "            Mds[threadIdx.y][threadIdx.x] = 0.0;\n",
        "        }\n",
        "\n",
        "        if (ph * TILE_WIDTH + threadIdx.y < M_cols && Col < N_cols) {\n",
        "            Nds[threadIdx.y][threadIdx.x] = d_N[(ph * TILE_WIDTH + threadIdx.y) * N_cols + Col];\n",
        "        } else {\n",
        "            Nds[threadIdx.y][threadIdx.x] = 0.0;\n",
        "        }\n",
        "\n",
        "        // Synchronize threads to ensure all threads have loaded their tiles\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute partial product for the tile\n",
        "        for (int k = 0; k < TILE_WIDTH; ++k) {\n",
        "            Pvalue += Mds[threadIdx.y][k] * Nds[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        // Synchronize threads to ensure all threads are done computing before loading the next tile\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the computed value to the output matrix\n",
        "    if (Row < M_rows && Col < N_cols) {\n",
        "        d_P[Row * N_cols + Col] = Pvalue;\n",
        "    }\n",
        "}\n",
        "\n",
        "void matrixMultiply(float *h_M, float *h_N, float *h_P, int M_rows, int M_cols, int N_cols) {\n",
        "    float *d_M, *d_N, *d_P;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc(&d_M, M_rows * M_cols * sizeof(float));\n",
        "    cudaMalloc(&d_N, M_cols * N_cols * sizeof(float));\n",
        "    cudaMalloc(&d_P, M_rows * N_cols * sizeof(float));\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_M, h_M, M_rows * M_cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_N, h_N, M_cols * N_cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block dimensions\n",
        "    dim3 dimBlock(TILE_WIDTH, TILE_WIDTH);\n",
        "    dim3 dimGrid((N_cols + TILE_WIDTH - 1) / TILE_WIDTH, (M_rows + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "    // Launch kernel\n",
        "    matrixMultiplyTiled<<<dimGrid, dimBlock>>>(d_M, d_N, d_P, M_rows, M_cols, N_cols);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(h_P, d_P, M_rows * N_cols * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_M);\n",
        "    cudaFree(d_N);\n",
        "    cudaFree(d_P);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define matrix dimensions\n",
        "    int M_rows = 32, M_cols = 64, N_cols = 32;\n",
        "\n",
        "    // Allocate and initialize host matrices\n",
        "    float *h_M = new float[M_rows * M_cols];\n",
        "    float *h_N = new float[M_cols * N_cols];\n",
        "    float *h_P = new float[M_rows * N_cols];\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (int i = 0; i < M_rows * M_cols; ++i) h_M[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    for (int i = 0; i < M_cols * N_cols; ++i) h_N[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    // Perform matrix multiplication\n",
        "    matrixMultiply(h_M, h_N, h_P, M_rows, M_cols, N_cols);\n",
        "\n",
        "    // Display result\n",
        "    std::cout << \"Matrix multiplication completed successfully.\" << std::endl;\n",
        "\n",
        "    // Free host memory\n",
        "    delete[] h_M;\n",
        "    delete[] h_N;\n",
        "    delete[] h_P;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo_LK0I7EwfV",
        "outputId": "84219728-2faa-4e89-e680-79b0c3a14492"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MatrixMul.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc MatrixMul.cu -o MatrixMul\n",
        "./MatrixMul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZHofWInecj9",
        "outputId": "e6ec5d1c-d017-48c5-ba73-793a6b3afbf6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multiplication completed successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGUYMP6cegM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}